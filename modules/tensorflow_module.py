from sklearn.model_selection import KFold
import tensorflow as tf
import numpy as np
from .. import settings
import json
import random
import datetime
import os
import math
import file_parsers
import time

class TensorFlowModule(object):
    def train(self):
        if not os.path.isfile(settings.samples_vectors_filepath):
            print('Missing samples vectors file!')
            file_parsers.parse_sample_files()
        # load the data from a file
        with open(settings.samples_vectors_filepath, 'r') as infile:
            print('Loading feature vectors... please wait!')
            sample_set = json.load(infile)
        self.run_training(sample_set)
    def run_training(self,sample_set):
        input_size = len(sample_set['features'])
        samples = [sample for sample in sample_set['samples']]
        x,W,b,pred,y,cost_func = self.create_model_vars_ops_placeholders(settings.n_classes,input_size)
        init = tf.global_variables_initializer()
        print('Initialized session variables!')
        saver = tf.train.Saver()
        config = tf.ConfigProto()
        config.gpu_options.allow_growth = True
        with tf.Session(config=config) as sess:
            print('Running session...')
            random.shuffle(samples)
            sample_batches = list(self.split_batches(samples,settings.batch_size))
            test_batch = sample_batches[0]
            train_batches = sample_batches[1:]
            sess.run(init)
            status = self.run_sess_with_epochs(cost_func,train_batches,sample_set,sess,x,y)
            if status == 1:
                print('Loss function failure... re-running session')
                tf.reset_default_graph()
                self.run_training(sample_set)
                return
            test_xs = [sample_set['samples'][sample]['vector'] for sample in test_batch]
            test_ys = [sample_set['samples'][sample]['is_malicious'] for sample in test_batch]
            print('Number of benign samples in test batch: ' + str(test_ys.count([0,1])))
            print('Number of malware samples in test batch: ' + str(test_ys.count([1,0])))
            tp, tn, fp, fn =sess.run(self.get_metrics_operations(pred,y),feed_dict={x: test_xs, y: test_ys})
            tpr, tnr, fpr, fnr, accuracy, recall, precision, f1_score = self.calculate_metrics(tp,tn,fn,fp)
            print('TPR: ' + str(tpr) + ' | ' + 'TNR: ' + str(tnr) + ' | ' + 'FPR: ' + str(fpr) + ' | ' + 'FNR: ' + str(fnr))
            print('Precision: ' + str(precision)  + ' | ' + 'Recall: ' + str(recall) + ' | ' + 'F1 Score:' + str(f1_score) + ' | ' + 'Accuracy: ', str(accuracy))
            print('Total number of samples: ' + str(len(samples)))
            saver.save(sess,os.path.join(settings.work_dir,'model'))
            with open('latest_train_logs.json', 'w') as outfile:
                json.dump({'timestamp':  '{:%Y-%b-%d %H:%M:%S}'.format(datetime.datetime.now()),
                           'n_files':  str(len(samples)), 'batch_size': str(settings.batch_size),
                           'n_train_batches':str(len(train_batches)),'benign_samples_test_batch': str(test_ys.count([0, 1])),
                          'malicious_samples_test_batch':str(test_ys.count([1, 0])), 'tpr': str(tpr),'tnr': str(tnr),
                          'fpr': str(fpr),'fnr': str(fnr),'accuracy': str(accuracy), 'precision': str(precision),'recall': str(recall),'f1_score': str(f1_score),}, outfile)
        tf.reset_default_graph()
    def calculate_metrics(self,tp,tn,fn,fp):
        tpr = float(tp) / (float(tp) + float(fn))
        tnr = float(tn) / (float(tn) + float(fp))
        fpr = float(fp) / (float(fp) + float(tn))
        fnr = float(fn) / (float(fn) + float(tp))
        accuracy = (float(tp) + float(tn)) / (float(tp) + float(fp) + float(fn) + float(tn))
        recall = tpr
        precision = float(tp) / (float(tp) + float(fp))
        f1_score = (2 * (precision * recall)) / (precision + recall)
        return [tpr,tnr,fpr,fnr,accuracy,recall,precision,f1_score]
    def get_metrics_operations(self,pred,output):
        predicted_output = tf.argmax(pred, 1)
        actual_output = tf.argmax(output, 1)
        actual_ones = tf.ones_like(actual_output)
        actual_zeros = tf.zeros_like(actual_output)
        predicted_ones = tf.ones_like(predicted_output)
        predicted_zeros = tf.zeros_like(predicted_output)
        tp_op = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(actual_output, actual_ones),
                                                     tf.equal(predicted_output, predicted_ones)), "float"))
        tn_op = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(actual_output, actual_zeros),
                                                     tf.equal(predicted_output, predicted_zeros)), "float"))
        fp_op = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(actual_output, actual_zeros),
                                                     tf.equal(predicted_output, predicted_ones)), "float"))
        fn_op = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(actual_output, actual_ones),
                                                     tf.equal(predicted_output, predicted_zeros)), "float"))
        return [tp_op, tn_op, fp_op, fn_op]
    def create_model_vars_ops_placeholders(self,n_classes, input_size):
        x = tf.placeholder(tf.float32, [None, input_size],name='x')
        W = tf.Variable(tf.zeros([input_size, n_classes]),name='W')
        b = tf.Variable(tf.zeros(n_classes),name='b')
        pred = tf.nn.softmax(tf.matmul(x, W) + b,name='pred')
        y = tf.placeholder(tf.float32, [None, n_classes],name='y')
        cost_func = -tf.reduce_sum(y * tf.log(pred),name='cost_func')
        return [x,W,b,pred,y,cost_func]
    def evaluate_model(self):
        tf.reset_default_graph()
        if not(os.path.isfile(settings.samples_vectors_filepath)):
            print('Missing samples vectors file!')
            file_parsers.parse_sample_files()
        print('Evaluating model')
        print('Loading feature vectors...')
        with open(settings.samples_vectors_filepath, 'r') as infile:
            sample_set = json.load(infile)
        samples = [sample for sample in sample_set['samples']]
        k_fold = KFold(settings.n_folds)
        ctr = 0
        eval_results={'timestamp':  '{:%Y-%b-%d %H:%M:%S}'.format(datetime.datetime.now()),'eval':{}}
        print('Total folds: ' + str(settings.n_folds))
        for train_indices, test_indices in k_fold.split(samples):
            ctr = self.run_evaluation_step(ctr,train_indices,test_indices,sample_set,eval_results)
        total_accuracy = 0
        total_recall = 0
        total_precision = 0
        total_f1_score = 0
        for key, value in (eval_results['eval']).items():
            total_accuracy += eval_results['eval'][key]['accuracy']
            total_precision += eval_results['eval'][key]['precision']
            total_f1_score += eval_results['eval'][key]['f1_score']
            total_recall += eval_results['eval'][key]['recall']
        macro_avg_accuracy = total_accuracy/settings.n_folds
        macro_avg_recall = total_recall/settings.n_folds
        macro_avg_precision = total_precision/settings.n_folds
        macro_avg_f1_score = total_f1_score/settings.n_folds
        eval_results['avg'] = {'accuracy':macro_avg_accuracy,'precision':macro_avg_precision,'recall':macro_avg_recall,'f1_score':macro_avg_f1_score}
        print('Macro-average Precision: ' + str(macro_avg_precision)  + ' | ' + 'Macro-average Recall: ' + str(macro_avg_recall) 
              + ' | ' + 'Macro-average F1 Score:' + str(macro_avg_f1_score) + ' | ' + 'Macro-average Accuracy: ', str(macro_avg_accuracy))
        with open('latest_eval_logs.json', 'w') as outfile:
            json.dump(eval_results, outfile)
    def run_evaluation_step(self,ctr,train_indices,test_indices,sample_set,eval_results):
        start_time = time.clock()
        samples = [sample for sample in sample_set['samples']]
        input_size = len(sample_set['features'])
        ctr += 1
        print('Split ' + str(ctr) + ' running')
        train_samples = []
        test_samples = []
        for index in train_indices:
            train_samples.append(samples[index])
        for index in test_indices:
            test_samples.append(samples[index])
        x, W, b, pred, y, cost_func = self.create_model_vars_ops_placeholders(settings.n_classes, input_size)
        init = tf.global_variables_initializer()
        status = 0
        with tf.Session() as sess:
            sess.run(init)
            train_batches = list(self.split_batches(train_samples, settings.batch_size))
            random.shuffle(train_batches)
            status = self.run_sess_with_epochs(cost_func, train_batches, sample_set, sess, x, y)
            if status != 1:
                test_xs = [sample_set['samples'][sample]['vector'] for sample in test_samples]
                test_ys = [sample_set['samples'][sample]['is_malicious'] for sample in test_samples]
                tp, tn, fp, fn = sess.run(self.get_metrics_operations(pred, y), feed_dict={x: test_xs, y: test_ys})
                tpr, tnr, fpr, fnr, accuracy, recall, precision, f1_score = self.calculate_metrics(tp,tn,fn,fp)
                print('TPR: ' + str(tpr) + ' | ' + 'TNR: ' + str(tnr) + ' | ' + 'FPR: ' + str(fpr) + ' | ' + 'FNR: ' + str(fnr))
                print('Precision: ' + str(precision)  + ' | ' + 'Recall: ' + str(recall) + ' | ' + 'F1 Score:' + str(f1_score) + ' | ' + 'Accuracy: ', str(accuracy))
                eval_results['eval'][str(ctr)] = {'tpr':tpr,'tnr':tnr,'fpr':fpr,'fnr':fnr,'accuracy':accuracy,'recall':recall,'precision':precision,'f1_score':f1_score}
        tf.reset_default_graph()
        if status == 1:
            print('Loss function failure... re-running step')
            self.run_evaluation_step(ctr-1,train_indices,test_indices,sample_set,eval_results)
        end_time = time.clock() - start_time
        print('Evaluation Time: ' + str("{:.4f}".format(end_time)))
        return ctr
    def test_model(self,type='test'):
        load_path = None
        if type is 'eval':
            load_path = settings.eval_savepath
            if not(os.path.isfile(load_path)):
                print('Missing samples vectors file!')
                file_parsers.parse_sample_files(type='eval')
        if type is 'test':
            load_path = settings.test_vectors
            if not(os.path.isfile(load_path)):
                print('Missing samples vectors file!')
                file_parsers.parse_test_file()
        print('Loading feature vectors...')
        with open(load_path, 'r') as infile:
            test_samples = json.load(infile)
        input_size = len(test_samples['features'])
        print('Number of test files: ' + str(len(test_samples['samples'])))
        x, W, b, pred, y, cost_func = self.create_model_vars_ops_placeholders(settings.n_classes, input_size)
        test_sample_names = [sha1 for sha1 in test_samples['samples']]
        test_sample_input = [test_samples['samples'][sha1]['vector'] for sha1 in test_samples['samples']]
        test_sample_output = None
        if type is 'eval':
            test_sample_output = [test_samples['samples'][sha1]['is_malicious'] for sha1 in test_samples['samples']]
        saver = tf.train.Saver()
        sample_verdicts = {}
        with tf.Session() as sess:
            print('Restoring trained model')
            saver.restore(sess, os.path.join(settings.work_dir,'model'))
            print('Testing...')
            if type is 'eval':
                print('Number of benign samples in test batch: ' + str(test_sample_output.count([0,1])))
                print('Number of malware samples in test batch: ' + str(test_sample_output.count([1,0])))
                tp, tn, fp, fn =sess.run(self.get_metrics_operations(pred,y),feed_dict={x: test_sample_input, y: test_sample_output})
                tpr, tnr, fpr, fnr, accuracy, recall, precision, f1_score = self.calculate_metrics(tp,tn,fn,fp)
                print('TPR: ' + str(tpr) + ' | ' + 'TNR: ' + str(tnr) + ' | ' + 'FPR: ' + str(fpr) + ' | ' + 'FNR: ' + str(fnr))
                print('Precision: ' + str(precision)  + ' | ' + 'Recall: ' + str(recall) + ' | ' + 'F1 Score:' + str(f1_score) + ' | ' + 'Accuracy: ', str(accuracy))
                print('Total number of samples: ' + str(len(test_sample_names)))
                with open('latest_test_logs.json', 'w') as outfile:
                    json.dump({'timestamp':  '{:%Y-%b-%d %H:%M:%S}'.format(datetime.datetime.now()),
                               'n_files':  str(len(test_sample_names)),
                               'benign_samples': str(test_sample_output.count([0, 1])),
                              'malicious_samples':str(test_sample_output.count([1, 0])),
                              'tpr': str(tpr),'tnr': str(tnr),'fpr': str(fpr),'fnr': str(fnr),
                              'accuracy': str(accuracy), 'precision': str(precision),'recall': str(recall),'f1_score': str(f1_score),}, outfile)
            else:
                for sha1 in test_sample_names:
                    test_sample_input = [test_samples['samples'][sha1]['vector']]
                    output = sess.run(pred,feed_dict={x:test_sample_input})
                    verd = sess.run(tf.equal(tf.constant(0, dtype=tf.int64),tf.argmax(output,axis=1)))
                    if verd[0] is np.True_:
                        sample_verdicts[sha1] ="Malicious"
                    else:
                        sample_verdicts[sha1] ="Clean"                
                    print(sha1 + ' is ' + sample_verdicts[sha1])
        tf.reset_default_graph()
        if type is 'test':
            return sample_verdicts
    def read_trained_stats(self):
        with open('latest_train_logs.json','r') as infile:
            stats = json.load(infile)
        return stats
    def read_test_stats(self):
        with open('latest_test_logs.json','r') as infile:
            stats = json.load(infile)
        return stats
    def run_sess_with_epochs(self,cost_func, train_batches, sample_set,tf_sess,x,y):
        print('Number of epochs: ' + str(settings.n_epochs))
        number_of_times = len(train_batches)
        print('Number of batches per epoch: ' + str(number_of_times))
        for z in range(settings.n_epochs):
            start_time = time.clock()
            avg_cost = 0
            for i in range(number_of_times - 1):
                decayed_learning_rate = tf.train.exponential_decay(settings.learning_rate, i, 1, settings.decay_rate, staircase=False)
                optimizer = tf.train.GradientDescentOptimizer(decayed_learning_rate).minimize(cost_func)
                batch_xs = [sample_set['samples'][sample]['vector'] for sample in train_batches[i]]
                batch_ys = [sample_set['samples'][sample]['is_malicious'] for sample in train_batches[i]]
                _, c = tf_sess.run([optimizer, cost_func], feed_dict={x: batch_xs, y: batch_ys})
                if math.isnan(c):
                    return 1
                else:
                    avg_cost += c / number_of_times
            print('Epoch ' + str(z) + ' cost: ' + str(avg_cost))
            end_time = time.clock() - start_time
            print('Time taken: ' + str("{:.4f}".format(end_time)))
    def split_batches(self,l, n):
        """Yield successive n-sized chunks from l."""
        for i in range(0, len(l), n):
            yield l[i:i + n]